Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 梯度下降

在绝大多数文章中，都以“一个人被困在山上，需要迅速下到谷底”来举例，这个人会“寻找当前所处位置最陡峭的地方向下走”。这个例子中忽略了安全因素，这个人不可能沿着最陡峭的方向走，要考虑坡度。

在自然界中，梯度下降的最好例子，就是泉水下山的过程：

<img src=".\Images\2\gd_water.png"/>

1. 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）
2. 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）
3. 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）


# 梯度下降的数学理解

先抛开神经网络，损失函数，反向传播等内容，用数学概念理解一下梯度下降。

梯度下降的数学公式：

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

其中：
- $\theta_{n+1}$：下一个值
- $\theta_n$：当前值
- $-$：梯度的反向
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长
- $\nabla$：梯度，函数当前位置的最快上升点
- $J(\theta)$：函数

## 梯度下降的三要素

1. 当前点
2. 方向
3. 步长

## 为什么说是“梯度下降”？

“梯度下降”包含了两层含义：
1. 梯度：函数当前位置的最快上升点
2. 下降：与导数相反的方向，用数学语言描述就是那个减号

亦即与上升相反的方向运动，就是下降。

<img src=".\Images\2\gd_concept.png"> 


## 单变量函数的梯度下降

假设一个单变量函数：

$$J(x) = x ^2$$

我们的目的是找到该函数的最小值，于是计算其微分：

$$J'(x) = 2x$$

假设初始位置为：

$$x_0=1.2$$

假设学习率：

$$\eta = 0.3$$

根据公式(1)，迭代公式：

$$x_{n+1} = x_{n} - \eta \cdot \nabla J(x)= x_{n} - \eta \cdot 2x\tag{1}$$

假设终止条件为J(x)<1e-2，迭代过程是：
```
x=0.480000, y=0.230400
x=0.192000, y=0.036864
x=0.076800, y=0.005898
x=0.030720, y=0.000944
```

上面的过程如下图所示：

<img src=".\Images\2\gd_single_variable.png"> 


## 双变量的梯度下降


假设一个双变量函数：

$$J(x,y) = x^2 + \sin^2(y)$$

我们的目的是找到该函数的最小值，于是计算其微分：

$${\partial{J(x,y)} \over \partial{x}} = 2x$$$${\partial{J(x,y)} \over \partial{y}} = 2\sin(y)\cos(y)$$

假设初始位置为：

$$(x_0,y_0)=(3,1)$$

假设学习率：

$$\eta = 0.1$$

根据公式(1)，迭代过程是的计算公式：
$$(x_{n+1},y_{n+1}) = (x_n,y_n) - \eta \cdot \nabla J(x,y) = (x_n,y_n) - \eta \cdot (2x,2 \cdot \sin y \cdot \cos y) \tag{1}$$

根据公式(1)，假设终止条件为J(x,y)<1e-2，迭代过程：
```
[3 1]
0: x=3.000000, y=1.000000, z=9.708073
     [6.         0.90929743]
[2.4        0.90907026]
1: x=2.400000, y=0.909070, z=6.382415
     [4.8        0.96956606]
[1.92       0.81211365]
2: x=1.920000, y=0.812114, z=4.213103
     [3.84      0.9985729]
[1.536      0.71225636]
...
15: x=0.105553, y=0.063481, z=0.015166
     [0.21110623 0.1266212 ]
[0.08444249 0.05081889]
16: x=0.084442, y=0.050819, z=0.009711
     [0.16888499 0.10146288]
```

上面的过程如下图所示，请注意看那条隐隐的黑色线，表示梯度下降的过程：

|观察角度1|观察角度2|
|---|---|
|<img src=".\Images\2\gd_double_variable.png">|<img src=".\Images\2\gd_double_variable2.png"> |


## 学习率η的选择

在code里，我们把学习率定义为learning_rate，或者eta，或者叫 $\alpha$。
针对上面的例子，当初始值为-0.8，学习率为1时，迭代的情况很尴尬：不断在一条水平线上跳来跳去，永远也不能下降。
<img src=".\Images\2\gd100.png"> 
当学习率=0.8时，会有这种左右跳跃的情况发生，这不利于神经网络的训练。
<img src=".\Images\2\gd080.png"> 
当学习率=0.6时，也会有跳跃，幅度偏小。
<img src=".\Images\2\gd060.png"> 
当学习率=0.4时，损失值会从单侧下降，4步以后基本接近了理想值。
<img src=".\Images\2\gd040.png"> 
当学习率=0.2时，损失值会从单侧下降，但下降速度较慢，8步左右接近极值。
<img src=".\Images\2\gd020.png"> 
当学习率=0.1时，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。
<img src=".\Images\2\gd010.png"> 

**练习：用Python代码实现以上过程**


