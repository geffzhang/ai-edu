Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 对标签值归一化

铁柱：再给你提个问题，你有没有注意，在计算Loss值时，会达到172.287，337.246这样大的数值，是什么原因呢？我们第四章的Loss值可都是小于1的数。

木头：哦！啊！这个......（喝了一口82年的雪碧压压惊）......我想想啊......

铁柱：提示你一下，第四章中的标签值也是[0,1]之间的。

木头：我明白了！我把这次的标签值也归一化试试看......（劈里啪啦敲键盘）.......（铁柱听着那机械键盘的声音觉得闹得慌）......（20分钟后）......老师，我在level5的基础上改了一下代码：

首先把X,Y分别归一化，公式如下：

$$y_{new} = \frac{y-y_{min}}{y_{max}-y_{min}} = \frac{y-y_{min}}{y_{range}} \tag{1}$$

代码如下：

```Python
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from level5_NormalizePredicateData import *

def NormalizeXYData(XData, YData):
    X, X_norm = NormalizeData(XData)
    Y, Y_norm = NormalizeData(YData)
    return X, X_norm, Y, Y_norm
```
如果只这样做的话，会得到非常小的W,B值：
```
w=0.01374991 -0.09151012  0.90392058
b=0.08233892
```
这样在预测时，z值也会非常小。所以要把预测出来的值也要做反归一化。根据公式1：

$$y = y_{new}*y_{range}+y_{min} \tag{2}$$

代码：
```Python
# try to give the answer for the price of west(2)，5th ring(5)，93m2
def PredicateTest(W, B, X_norm, Y_norm):
    xt = np.array([2,5,93]).reshape(3,1)
    xt_new = NormalizePredicateData(xt, X_norm)
    z = ForwardCalculationBatch(W, B, xt_new)
    zz = z * Y_norm[1,0] + Y_norm[0,0]
    return zz
```

倒数第二行代码，就是公式2。在主程序里，要把X,Y同时归一化：

```Python
# main
if __name__ == '__main__':
    # hyper parameters
    # SGD, MiniBatch, FullBatch
    method = "SGD"
    # read data
    raw_X, raw_Y = ReadData()
    X, X_norm, Y, Y_norm = NormalizeXYData(raw_X, raw_Y)
    w, b = train(method, X, Y)
    z = PredicateTest(w, b, X_norm, Y_norm)
    print("z=", z)

```

运行......结果如下：
```
[[ 0.01374991 -0.09151012  0.90392058]] [[0.08233892]]
epoch=0, iteration=998, loss=0.000000
z= [[529.00148631]]
```
哈哈！82年的雪碧果然厉害！我们只迭代了1轮就结束了，而且得到了正确的预测值！

铁柱：不错不错！我们来总结一下正确的归一化和反归一化的关系：

|归一化|Weight|Bias|预测值Xt|预测方法|
|---|---|---|---|---|
|只归一化X|$W_{norm}$反归一化为$W_{real}$|$W_{norm}$反归一化为$B_{real}$|不归一化$Xt$|用$W_{real},B_{real}$预测|
|只归一化X|使用训练结果$W_{norm}$|使用训练结果$B_{norm}$|归一化为$Xt_{norm}$|用$W_{norm},B_{norm}$预测|
|同时归一化X和Y|使用训练结果$W_{norm}$|使用训练结果$B_{norm}$|归一化为$Xt_{norm}$|用$W_{norm},B_{norm}$预测,再反归一化结果|

木头：哦！这样一总结，就可以一目了然啦！我也简单总结一下：

1. X必须归一化，否则无法训练
2. 训练出的结果W和B，在推理时有两种使用方式：
a. 直接使用，此时必须把推理时输入的X也做相同规则的归一化
b. 反归一化为W,B的本来值$W_{Real},B_{Real}$，推理时输入的X不需要改动
3. Y可以归一化，好处是迭代次数少。如果结果收敛，也可以不归一化，如果不收敛（数值过大），就必须归一化
4. 如果Y归一化，先沿袭第2步的做法，对得出来的结果做关于Y的反归一化

铁柱：可以看到对标签值的归一化确实给我们带来了好处，迭代次数减少了一倍多。其实，咱们这个网络还很简单，这样折腾一下是为了让大家有深刻的理解，这里有两张图可以对比：

|标签值归一化|标签值不归一化|
|------|-----|
|<img src=".\Images\5\LossWithNormalizeLabelData.png">|<img src=".\Images\5\LossWithoutNormalizeLabelData.png">|

左侧做了标签值归一化，看loss值起始值0.035，看log用了900次迭代。

右侧的没有做标签值归一化，loss从4000开始，用了1200多次迭代，达到了理想值。

真正复杂的神经网络有个理论叫做"Batch Normalization"——批标准化，以后咱们遇到时再讲。

木头：好啊好啊！

至此，我们完美地解决了北京地区的房价预测问题！但是还没有解决自己可以有能力买一套北京的房子的问题......

代码位置：ch06, level6