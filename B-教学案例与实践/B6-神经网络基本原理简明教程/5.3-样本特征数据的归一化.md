Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 解决训练不收敛问题

仔细分析一下屏幕打印信息：
```
0 0 48904435754.68428 [[  93.8  187.6 3705.1]] [[46.9]]
0 1 1.8166533368107932e+16 [[  -57809.94  -115619.88 -2254540.76]] [[-28904.97]]
```
前两次迭代的损失值已经是天文数字了，后面的W和B的值也在不断变大，说明网络发散了。

[参考资料: My Nueral Network isn't working! What should I do?](http://theorangeduck.com/page/neural-network-not-working)

难度我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。别慌，擦干净头上的冷汗，踩着自己的尸体继续前行！


回想一个问题：为什么在第4章中，我们没有遇到这种情况？把样本拿来看一看：

|样本序号|1|2|3|...|200|
|---|---|---|---|---|---|
|服务器数量(千)|0.928|0.0469|0.855|...|0.373|
|空调功率(千瓦)|4.824|2.950|4.643|...|3.594|

因为所有的X值（服务器数量）都是在[0,1]之间的，而本次的数据有三个特征值，全都是不是在[0,1]之间的，并且取值范围还不相同。我们不妨把本次样本数据也做一下这样的处理，亦即“归一化”。

**其实，数据归一化是深度学习的必要步骤之一，已经是魔法师们家喻户晓的技能，也因此它很少被各种博客/文章所提及，以至于麻瓜们经常被坑。**

# 为什么要做归一化

木头：老师，归一化原来这么重要，在理论上是怎么解释呢？和对飞行员的身高要求是一个道理吗？

铁柱：理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说：

1. 样本的各个特征的取值要符合概率分布，即[0,1]
2. 样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！
3. 神经网络假设所有的输入输出数据都是标准差为1，均值为0，包括权重值的初始化，激活函数的选择，以及优化算法的的设计。

4. 数值问题

    归一化/标准化可以避免一些不必要的数值问题。因为sigmoid/tanh的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tanh( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。

5. 初始化
    
    在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
    
6. 梯度
    
    以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是误差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
    
7. 学习率：
   
    知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
    

参考链接：https://www.jianshu.com/p/95a8f035c86c


木头：是不是说在房价数据中，地理位置的取值范围是[2,6]，而房屋面积的取值范围为[40,120]，二者相差太远，根本不可以放在一起计算了？

铁柱：是的，你看$W_1X_1+W_2X_2$这个式子中，如果X1的取值是[2,6]，X2的取值是[40,120]，相差太远，而且都不在[0,1]之间，所以对于神经网络来说很难理解。下图展示了归一化前后的情况Loss值的等高图，意思是地理位置和房屋面积取不同的值时，作为组合来计算损失函数值时，形成的类似地图的等高图。左侧为归一化前，右侧为归一化后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

## 几个基本数学概念

木头：我还听说有一些类似的词汇，什么标准化，这个和归一化有什么区别呢？

铁柱：先看几个基本概念：

- 均值mean：

$$\bar{x}=\frac{1}{n}\sum_i^nx_i \tag{1}$$

- 标准差stdandard deviation：

$$
std=\sqrt{\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}} \tag{2}
$$

- 方差variance

$$
var=\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}=s^2 \tag{3}
$$


- 协方差covariance
$$
cov(X,Y)=\frac{1}{n-1} \sum_i^n{[(x_i-\bar{x})(y_i-\bar{y})]} \tag{4}
$$
结果为正，表示X,Y是正相关。


铁柱：有三个类似的概念，归一化，标准化，中心化。

## 归一化

把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。

归一化有三种方法：
- Min-Max归一化：
$$x_{new}={x-MinValue \over MaxValue - MinValue} \tag{5}$$
- 平均值归一化： 
$$x_{new} = {x - \bar{x} \over MaxValue - MinValue} \tag{6}$$

- 非线性归一化：
$$对数转换y=log(x)，反余切转换y=atan(x) \cdot 2/π  \tag{7}$$

## 标准化
把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。

Z-score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：
$$x_{new} = (x - \bar{x})／std \tag{8}$$

## 中心化

平均值为0，无标准差要求：
$$x_{new} = x - \bar{x} \tag{9}$$


# 如何做数据归一化

## 样本分析

再把这个表拿出来分析一下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|价格（万元）|434|500|321|482|...|410|

我们用min-max标准化来归一以上的样本特征数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|0|0.667|0.333|1|...|0.33|
|地理位置（几环）|0.25|0|1|0.25|...|0.75|
|居住面积（平米）|0.7|0.75|0.175|0.4|...|0.36|
|价格(万元)|434|500|321|482|...|410|

注意：
1. 我们并没有归一化样本的标签Y数据，所以最后一行的价格还是保持不变
2. 我们是对三行特征值分别做归一化处理的

## 代码实现
我们服用level2_NeuralNetwork.py中的所有函数，需要在python文件最前面加上如下声明：
```Python
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from level2_NeuralNetwork import *
```

把归一化的函数写好：
```Python
# normalize data by extracting range from source data
def NormalizeData(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    x_range = np.zeros((1,n))
    x_min = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        max_value = np.max(x)
        min_value = np.min(x)
        x_min[0,i] = min_value
        x_range[0,i] = max_value - min_value
        x_new = (x - x_min[0,i])/(x_range[0,i])
        X_new[i,:] = x_new
    return X_new, x_range, x_min
```
返回值：
- X_new：归一化后的样本，是个矩阵
- x_range：表示样本特征值的取值范围，比如80，从120-40=80得到
- x_min：表示样本特征值的取值范围的下限，比如40


主程序修改一下：
```Python
# 主程序
if __name__ == '__main__':
    # hyper parameters
    # SGD, MiniBatch, FullBatch
    method = "SGD"

    eta, max_epoch,batch_size = InitializeHyperParameters(method)
    
    # W size is 3x1, B is 1x1
    W, B = InitialWeights(3,1,2)
    # calculate loss to decide the stop condition
    error = 1e-5
    loss = 10 # set to any number bigger than error value
    dict_loss = {}
    # read data
    raw_X, Y = ReadData()
    X,X_range,X_min = NormalizeData(raw_X)
    # count of samples
    num_example = X.shape[1]
    num_feature = X.shape[0]

    # if num_example=200, batch_size=10, then iteration=200/10=20
    max_iteration = (int)(num_example / batch_size)
    for epoch in range(max_epoch):
        print("epoch=%d" %epoch)
        for iteration in range(max_iteration):
            # get x and y value for one sample
            batch_x, batch_y = GetBatchSamples(X,Y,batch_size,iteration)
            # get z from x,y
            batch_z = ForwardCalculationBatch(W, B, batch_x)
            # calculate gradient of w and b
            dW, dB = BackPropagationBatch(batch_x, batch_y, batch_z)
            # update w,b
            W, B = UpdateWeights(W, B, dW, dB, eta)
            
            # calculate loss for this batch
            loss = CheckLoss(W,B,X,Y)
            print(epoch,iteration,loss,W,B)
            dict_loss[loss] = CData(loss, W, B, epoch, iteration)            
        if loss < error:
            break

    ShowLossHistory(dict_loss, method)
    w,b,cdata = GetMinimalLossData(dict_loss)
    print(cdata.w, cdata.b)
    print("epoch=%d, iteration=%d, loss=%f" %(cdata.epoch, cdata.iteration, cdata.loss))
```

用颤抖的双手同时按下Ctrl+F5，运行开始，一眨眼，结束！如白驹过隙！

仔细看打印结果：
```
epoch=0
0 0 86628.81263808327 [[16.66625102 24.35378204 22.90415924]] [[46.82833874]]
0 1 62384.512258782626 [[29.61365644 43.77489018 41.58775694]] [[85.67055502]]
0 2 33542.928682862184 [[44.76948514 89.24237628 85.32862964]] [[131.13804111]]
......
1 997 1.7676953228346485e-08 [[  5.9998435  -40.00018471 394.99950506]] [[292.00051208]]
1 998 1.5818738060552483e-08 [[  5.99981695 -40.00019134 394.99950271]] [[292.00048552]]
1 999 1.5957876255087813e-08 [[  5.99982024 -40.00019011 394.99950634]] [[292.00049047]]
w= [[  5.99981695 -40.00019134 394.99950271]] 
b= [[292.00048552]]
epoch=1, iteration=998, loss=0.000000
```
由于我们在主程序中新增加了一个停止条件：
```Python
        if loss < error:    # 1e-5
            break
```
所以网络在第二个epoch就停止训练了，收敛了！看一下损失函数图像：

<img src=".\Images\5\correct_loss.png">

木头：Perfect!!!老师，我们成功啦！

铁柱：（白了木头一眼）别那么鸡冻！看看w和b的输出值再说！

木头：（定睛一看，傻眼了）啊，我记得正规方程的解是：
```
w1=2.000000,w2=-10.000000,w3=5.000000,b=110.000000
```
而神经网络是：
```
w= [[  5.99981695 -40.00019134 394.99950271]] 
b= [[292.00048552]]
```
这...这...差距有点儿大哦！哪里有问题呢？我把两组数据列出来，回去研究一下：

|方法|W1|W2|W3|B|预测结果
|---|---|---|---|---|---|
|正规方程|2|-10|5|110|529万元
|神经网络|5.99|-40.00|394.99|292.00|36838万元|

样本数据：

|特征|朝向|位置|面积|
|----|----|---|---|
|最小值|1|2|40|
|最大值|4|6|119|
|待预测房子参数|2|5|93|

正规方程预测房价结果：

$$
Z=WX+B=2 \times 2 - 10 \times 5 + 5 \times 93 + 110 = 529(万元)
$$

神经网络预测房价结果：

$$
Z=WX+B=5.99 \times 2 - 40 \times 5 + 394.99 \times 93 + 292 = 36,838(万元)
$$

代码位置：ch05/level3