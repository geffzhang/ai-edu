Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

# 可视化训练结果

木头：老师，我觉得您最后展示的那张图中的两条分割线，明确地显示训练结果，我也看了code，但想知道这是怎么原理呢？

<img src=".\Images\6\eps1e-10.png">

铁柱：可视化的训练结果会对我们的调试、学习有巨大的帮助。假设我们有如下训练结果：

```
W:
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
B:
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```
以这个训练结果为例，回忆一下前向计算的过程：

$$
Z = W*X+B
$$
实际上是以下计算的组合：

$$
Z1 = W_1 * X + B_1=\begin{pmatrix}w_{11} & w_{12}\end{pmatrix} * \begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b_1=w_{11} * x_1 + w_{12} * x_2 + b_1
$$
$$
Z2 = W_2 * X + B_2=\begin{pmatrix}w_{21} & w_{22}\end{pmatrix} * \begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b_2 = w_{21} * x_1 + w_{22} * x_2 + b_2 
$$
$$
Z3 = W_3 * X + B_3=\begin{pmatrix}w_{31} & w_{32}\end{pmatrix} * \begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b_3=w_{31} * x_1 + w_{32} * x_2 + b_3
$$

木头：Z1，Z2，Z3，分别代表了三个区域，如果确定了区域的边界，那就是分割线了。

铁柱：思路正确！如果想画出Z1/Z2之间的分界线，也就是说，在这条分界线上的点，即可能属于Z1，又可能属于Z2，那么令Z1 = Z2就可以了：

$$
Z1 = Z2 \\
w_{11} * x_1 + w_{12} * x_2 + b1 = w_{21} * x_1 + w_{22} * x_2 + b2 \\
x_2 = \frac{(w_{21}-w_{11}) * x_1 + b2-b1}{w_{12} - w_{22}}
$$
同理，Z2/Z3的分界线是：

$$
Z2 = Z3 \\
x_2 = \frac{(w_{31}-w_{21})*x_1+b3-b2}{w_{22}-w_{32}}
$$

然后把求得的W,B的矩阵值分别代入就可以了。这是上图中那条绿色的分割线：

$$
x_2 = \frac{(2.865-0.224)*x_1+0.950-8.513}{-18.076-0.276} \\
x_2 = -0.144x_1 + 0.412
$$

这是上图中红色的分割线：

$$
x_2 = \frac{(-3.089-2.865)*x_1-9.463-0.950}{0.276-17.799} \\
x_2 = 0.339x_1 + 0.594
$$

用Python代码实现：

```Python
def ShowResult(X,Y,W,B,rangeX,eta,iteration,eps):
    for i in range(X.shape[1]):
        if Y[0,i] == 0 and Y[1,i]==0 and Y[2,i]==1:
            plt.scatter(X[0,i], X[1,i], c='r')
        elif Y[0,i] == 0 and Y[1,i]==1 and Y[2,i]==0:
            plt.scatter(X[0,i], X[1,i], c='b')
        else:
            plt.scatter(X[0,i], X[1,i], c='g')
   
    b12 = (B[1,0] - B[0,0])/(W[0,1] - W[1,1])
    w12 = (W[1,0] - W[0,0])/(W[0,1] - W[1,1])

    b23 = (B[2,0] - B[1,0])/(W[1,1] - W[2,1])
    w23 = (W[2,0] - W[1,0])/(W[1,1] - W[2,1])

    x = np.linspace(0,rangeX,10)
    y = w12 * x + b12
    plt.plot(x,y)

    x = np.linspace(0,rangeX,10)
    y = w23 * x + b23
    plt.plot(x,y)

    title = str.format("eta:{0}, iteration:{1}, eps:{2}", eta, iteration, eps)
    plt.title(title)
    
    plt.xlabel("Temperature")
    plt.ylabel("Humidity")
    plt.show()
```

# Softmax函数的Python实现

木头：老师，我注意到了Softmax函数的样实现，最后一行代码里有个axis=0，这个以前没见过。
```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z, axis=0)
    return A
```

铁柱：我最开始也没注意到这一点，没有添加那个参数，但是得到的loss值和diff_loss值总居高不下，从来不会到达diff_loss < 1e-10而结束训练的效果。于是开启Debug大法，看看每一步都是怎么回事儿！

由于是随机梯度下降，每次只使用一个样本，所以在主程序里计算Softmax(Z)时是这样的值：
```
Z
array([[0.        ],
       [0.        ],
       [0.37513412]])

A
array([[0.28941997],
       [0.28941997],
       [0.42116006]])
```
得到的A值无比正确！说明我们的Softmax函数正常工作了。

然后进入下面的check_diff_cross函数，在里面又调用了一次Softmax函数，值是这样的：
```
Z
array([[0.39984422, 0.36663887, 0.39279115, 0.40192722, 0.40127821,
        0.38525265, 0.38244349, 0.36781724, 0.35705948, 0.39823856,
        0.37349707, 0.39216078, 0.38434552, 0.36263786, ...
 
A
array([[0.0018422 , 0.00178204, 0.00182926, 0.00184605, 0.00184485,
        0.00181552, 0.00181043, 0.00178414, 0.00176505, 0.00183925,
        0.0017943 , 0.0018281 , 0.00181387, 0.00177492, ...,
```
因为Z和A都是3x200的数组，所以我们只看一组数值：
```
Z[:,0]: array([0., 0., 0.49756787])
A[:,0]: array([0.00139904, 0.00139904, 0.00230102])
```
木头：欸！不对欸！A应该是Z的Softmax结果，应该是：
```
[0.27436978, 0.27436978, 0.45126044]，
```
但debug出来却是：
```
[0.00139904 , 0.00139904, 0.00203132]
```
铁柱：这个明显不对！再Debug进入Softmax函数，检查到这一行：
```Python
A = exp_Z / np.sum(exp_Z)
```
后面的np.sum(exp_Z)的值等于405.3137756，是一个标量，然后计算出来的A就变成了exp_Z矩阵整个除以一个标量，这已经不是Softmax的原意了，应该是exp_Z只除以和自己相关的列的和才有意义，所以np.sum(exp_Z)应该也是一个数组才对，应该是一个1x200的数组，代表了每个样本（每列是一个样本）的sum值。

思路有了，改一下代码：
```Python
A = exp_Z / np.sum(exp_Z, axis=0)
```
axis=0是重点，它的意思是要把exp_Z针对每一列的3个值单独求和，得出一个200个元素的数组，代表了200个Z数据的每一列的和。图示如下：

$$
expZ = 
\begin{pmatrix}
z_{1,1} & z_{1,2} \dots z_{1,200} \\
z_{2,1} & z_{2,2} \dots z_{2,200} \\
z_{3,1} & z_{3,2} \dots z_{3,200} \\
\end{pmatrix}
$$

np.sum(expZ, axis=0)的结果：

$$
sum = \begin{pmatrix} (z_{1,1}+z_{2,1}+z_{3,1}) & (z_{1,2}+z_{2,2}+z_{3,2}) \dots (z_{1,200}+z_{2,200}+z_{3,200}) \end{pmatrix}
$$

$$ 
=\begin{pmatrix}
z1 & z2 \dots z200
\end{pmatrix}
$$

然后再执行A = exp_Z / np.sum(exp_Z, axis=0)这一行代码，就会得到正确的A数组值。再看一下A[:,0]的结果：
```
[0.27436978 0.27436978 0.45126044]
```
木头：Yes! Exactly what we want!

铁柱：（翻了个白眼——拽英文！）运算后可以看到Loss值不断在0.112附近徘徊，而diff_loss值不断减小，终于小于1e-10了！

# 提高训练准确度
木头：还有个问题，在下面那个图中，还可以看到有些点没有完全分割对，但是我们已经结束了迭代，这个怎么处理？能得到更好的训练结果吗？

铁柱：eps=1e-10是我们的一个期望，是根据以前的经验设置的，意思是说达到了这个指标后，神经网络的训练基本可以结束了，但是具体问题还要具体分析，你可以分别试试更大的值和更小的值，看看有什么规律可循？

木头：吼的！我分别试试1e-5，1e-8，1e-10，1e-11吧！......（五分钟后）......出来啦，放在表格里便于大家对比。

||||
|---|---|---|
||<img src=".\Images\6\eps1e-5.png">|<img src=".\Images\6\eps1e-8.png">|
||<img src=".\Images\6\eps1e-10.png">|<img src=".\Images\6\eps1e-11.png">|

铁柱：可以看到，eps越大，训练时间越短，但是结果越差。比如第一张图只迭代了两轮就结束了。eps越小，越有向理想值趋近的趋势，第4张图比前者更准确地把三种颜色分开了，但是它迭代了397轮才结束。天下没有免费的午餐啊！


