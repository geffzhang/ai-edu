Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
## relu函数

形式：

$$f(z) = \begin{cases} z & z \geq 0 \\ 0 & z < 0 \end{cases}$$

反向传播:

$$f(z) = \begin{cases} 1 & z \geq 0 \\ 0 & z < 0 \end{cases}$$

<img src=".\Images\7\relu.png">


先来说说神经学方面的解释，为什么要使用relu呢？要说模仿神经元，sigmoid不是更好吗？这就要看2001年神经学家模拟出的更精确的神经元模型[Deep Sparse Rectiﬁer Neural Networks](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)。简单说来，结论就是这样两幅图：


<img src=".\Images\7\new_bio.png">

<img src=".\Images\7\nearRelu.png">


下面来解释函数图像：在输入的信号比0大的情况下，直接将信号输出，否则的话将信号抑制到0，相比较于上面两个激活函数，relu计算方面的开销非常小，避免了指数运算和除法运算。此外，很多实验验证了采用relu函数作为激活函数，网络收敛的速度可以更快。至于收敛更加快的原因，从图上的梯度计算可以看出，relu的反向传播梯度恒定是1，而sigmoid激活函数中大多数时间梯度是比1小的。在叠加很多层之后，由于链式法则的乘法特性，sigmoid作为梯度函数会不断减小反向传播的梯度，而relu可以将比梯度原样传递，也就是说，relu可以使用比较快的速度去进行参数更新。  

用和sigmoid函数那里更新相似的算法步骤和参数，来模拟一下relu的梯度下降次数，也就是学习率$\alpha = 0.2$，希望函数值从0.9衰减到0.5，这样需要多少步呢？

<img src=".\Images\7\decay_relu.png">

也就是说，同样的学习速率，relu函数只需要两步就可以做到sigmoid需要67步才能衰减到的程度！

但是如果回传了一个很大的梯度导致网络更新之后输入信号小于0了呢？那么这个神经元之后接受到的数据是零，对应新的回传的梯度也是零，这个神经元将不被更新，下一次输入的信号依旧小于零，不停重复这个过程。也就是说，这个神经元不会继续更新了，这个神经元“死”掉了。在学习率设置不恰当的情况下，很有可能网络中大部分神经元“死”掉，也就是说不起作用了，而这是不可取的。

代码实现：  
与sigmoid类似，relu函数的前向传播和反向传播与输入的大小有关系，小于0的输入可以被简单的置成0，不小于0的可以继续向下传播，也就是将输入和0中较大的值继续传播，对输入向量逐元素做比较即可。考虑到反向传播时梯度计算也和输入有关，使用一个mask对数据或者根据数据推出的反向传播结果做一个记录也是一个比较好的选择。

示例代码:

```python
class Crelu(object):
    def __init__(self, inputSize):
        self.shape = inputSize

    def forward(self, image):
        # 用于记录传递的结果
        self.mask = np.zeros(self.shape)
        self.mask[image > 0] = 1
        # 将小于0的项截止到0
        return np.maximum(image, 0)

    def gradient(self, preError):
        # 将上一层传递的误差函数和该层各位置的导数相乘
        return np.multiply(preError, self.mask)
```

想想看，relu函数的缺点是什么呢？是梯度很大的时候可能导致的神经元“死”掉。而这个死掉的原因是什么呢？是因为很大的梯度导致更新之后的网络传递过来的输入是小于零的，从而导致relu的输出是0，计算所得的梯度是零，然后对应的神经元不更新，从而使relu输出恒为零，对应的神经元恒定不更新，等于这个relu失去了作为一个激活函数的梦想。问题的关键点就在于输入小于零时，relu回传的梯度是零，从而导致了后面的不更新。

那么最简单粗暴的做法是什么？在输入函数值小于零的时候给他一个梯度不就好了！这就是leaky relu函数的表现形式了！

## leaky relu函数

形式：

$$f(z) = \begin{cases} z & z \geq 0 \\ \alpha * z & z < 0 \end{cases}$$

反向传播:

$$f(z) = \begin{cases} z & 1 \geq 0 \\ \alpha & z < 0 \end{cases}$$

<img src=".\Images\7\leakyRelu.png">

相比较于relu函数，leaky relu同样有收敛快速和运算复杂度低的优点，而且由于给了$x<0$时一个比较小的梯度$\alpha$,使得$x<0$时依旧可以进行梯度传递和更新，可以在一定程度上避免神经元“死”掉的问题。

示例代码：

```python
class CleakyRelu(object):
    def __init__(self, inputSize, alpha):
        self.shape = inputSize
        self.alpha = alpha

    def forward(self, image):
        # 用于记录传递的结果,按照传递公式生成对应的值
        self.mask = np.zeros(self.shape)
        self.mask[image > 0] = 1
        self.mask[image <= 0] = self.alpha
        # 将该值对应到输入中
        return np.multiply(image, self.mask)

    def gradient(self, preError):
        # 将上一层传递的误差函数和该层各位置的导数相乘
        return np.multiply(preError, self.mask)
```
