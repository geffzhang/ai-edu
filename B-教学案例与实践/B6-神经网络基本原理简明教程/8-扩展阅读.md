Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

“铁柱”是一名老师，在神经网络中穿梭多年，挂了满身的蜘蛛网。“木头”是一名刚入门者，木头木脑的，有问题经常向铁柱请教。

# 神经网络的设计

木头：老师，我总结一下本章学习到的内容。

在本章中，为了完成函数拟合任务，我们使用如下正向过程：

$$Z1=W1 \cdot X+B1$$

$$A1=sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$

以及这个损失函数：

$$Loss = \frac{1}{2}(Z - Y) ^ 2 = \frac{1}{2}\sum_i(z_i-y_i)^2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

铁柱：总结得很好！

木头：但是这里有两个疑问：

1. 为什么在神经网络的第二层，没有用到激活函数？
2. 为什么我们用均方差损失函数，而不是交叉熵损失函数？

铁柱：这先要理解神经网络的原理。首先，一层神经网络肯定不能完成这个复杂函数的拟合过程，而两层神经网络，只要神经元的数量合适，肯定能完成这个任务。

铁柱：其次，神经网络的拟合原理是这样的：在第一层神经网络，通过W1*X的计算做线性变化，把非线性问题转换成线性问题；在第二层神经网络做线性回归。所以在第二层是不需要激活函数的，否则就没法画出一条直线来。这个可以想象两个独立的神经网络，第一个网络以及把数据处理成线性的了，我们使用第4章的方法，做一次线性回归就好了。

铁柱：第三，由于是回归问题，所以我们仍然使用均方差损失函数。交叉熵损失函数是为了分类问题而设计的。

# 反向传播的求导过程

木头：由于是第一次接触两层的神经网络，我看到反向传播的函数体中，比起一层的网络复杂了很多。您能详细解释一下吗？

铁柱：看一下计算图，然后用链式求导法则反推。

<img src=".\Images\8\Back.jpg" width="800">

蓝色的箭头线表示正向计算过程，黄色的箭头线表示反向的传播过程，其中dLoss/dZ2就是下面公式中的$\partial{Loss}\over{\partial{Z2}}$。

## 求W2的梯度
因为：

$$Loss = \frac{1}{2}(Z2-Y2)^2$$
$$Z2 = W2 \cdot A1+B2$$

所以我们用Loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{Loss}}{\partial{W2}} = \frac{\partial{Loss}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{W2}}
$$

其中：

$$
\frac{\partial{Loss}}{\partial{Z2}} = \frac{\partial{}}{\partial{Z2}}[\frac{1}{2}(Z2-Y2)^2] = Z2-Y => dZ2 \tag{公式一}
$$

而：

$$
\frac{\partial{Z2}}{\partial{W2}} = \frac{\partial{}}{\partial{W2}}(W2 \cdot A1+B2) = A1^T
$$

所以：

$$
\frac{\partial{Loss}}{\partial{W2}} = \frac{\partial{Loss}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{W2}} = (Z2-Y) \cdot A1^T \tag{公式二}
$$

矩阵求导的理论部分较为复杂，请大家参考我们的《基本数学导数公式》章节。

## 求B2的梯度

$$
\frac{\partial{Loss}}{\partial{B2}} = \frac{\partial{Loss}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{B2}}
$$

其中第一项前面算w的时候已经有了，而：

$$
\frac{\partial{Z2}}{\partial{B2}} = \frac{\partial{(W2 \cdot A1+B2)}}{\partial{B2}} = 1
$$

所以：

$$
\frac{\partial{Loss}}{\partial{B2}} = \frac{\partial{Loss}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{B2}} = Z2-Y \tag{公式三}
$$

## 求W1的梯度
因为：

$$A1 = sigmoid(Z1)$$

$$Z1 = W1 \cdot X+B1$$

对Z1求导：

$$
\frac{\partial{Loss}}{\partial{Z1}} = \frac{\partial{Loss}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{A1}} \cdot \frac{\partial{A1}}{\partial{Z1}}
$$

其中前面推导过：

$$
\frac{\partial{Loss}}{\partial{Z2}} = Z2-Y =>dZ2 \tag{令结果为dZ2，以便后面使用}
$$

而：

$$
\frac{\partial{Z2}}{\partial{A1}} = \frac{\partial{}}{\partial{A1}}(W2 \cdot A1+B2) = W2^T
$$

$$
\frac{\partial{A1}}{\partial{Z1}} = \frac{\partial{}}{\partial{Z1}}(sigmoid(Z1)) = A1 \cdot (1-A1) \tag{公式四}
$$
对sigmoid函数的求导请看我们的《基本数学导数公式》或者《激活函数》章节。

所以：

$$
\frac{\partial{Loss}}{\partial{Z1}} = W2^T \cdot dZ2 \cdot A1 \cdot (1-A1) => dZ1\tag{公式五}
$$
而W1,B1的求导结果和W2,B2类似：

$$
\frac{\partial{Loss}}{\partial{W1}} = \frac{\partial{Loss}}{\partial{Z1}} \cdot \frac{\partial{Z1}}{\partial{W1}}=dZ1 \cdot \frac{\partial{(W1 \cdot X+B1)}}{\partial{W1}}=dZ1 \cdot X^T \tag{公式六}
$$

$$
\frac{\partial{Loss}}{\partial{B1}} = \frac{\partial{Loss}}{\partial{Z1}} \cdot \frac{\partial{Z1}}{\partial{B1}}=dZ1 \cdot \frac{\partial{(W1 \cdot X+B1)}}{\partial{B1}}=dZ1 \tag{公式七}
$$

变成代码：
```Python
def BackPropagation(x, y, dictCache, dictWeights):
    A1 = dictCache["A1"]
    A2 = dictCache["A2"]
    W2 = dictWeights["W2"]
    # 公式一 dZ2
    dLoss_Z2 = (A2 - Y)
    # 公式二
    dW2 = np.dot(dLoss_Z2, A1.T)
    # 公式三
    dB2 = np.sum(dLoss_Z2, axis=1, keepdims=True)
    # 中间步骤
    dLoss_A1 = np.dot(W2.T, dLoss_Z2)
    # 公式四
    dA1_Z1 = A1 * (1 - A1)
    # 公式五
    dLoss_Z1 = dLoss_A1 * dA1_Z1
    # 公式六
    dW1 = np.dot(dLoss_Z1, X.T)
    # 公式七
    dB1 = np.sum(dLoss_Z1, axis=1, keepdims=True)

    dictGrads = {"dW1":dW1, "dB1":dB1, "dW2":dW2, "dB2":dB2}
    return dictGrads
```
在计算dB2和dB1的时候，我们注意到参数中含有"axis=1, keepdims=True"，这是为了可以批量数据反向传播时，正确地计算dB的值。

# 参数调整

木头：老师，经常听人说起“调参”，很神秘的样子。

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

咱们的基准代码中的参数设置如下：
1. 隐藏层神经元数=128
2. 输入训练数据量=1000
3. 学习率=0.1
4. 损失函数值<=0.002，也就是能基本拟合住目标曲线的前提下

## 中间层神经元数量的影响

神经元数量=96,128，设置代码中的n_hidden=96或者128。

|neuron=96,iteraion=3966|neuron=128,iteration=983|
|---|---|
|<img src=".\Images\8\r96-1000.png">|<img src=".\Images\8\r128-1000.png">|

结论：基准神经元数为128，在96时，需要迭代3966次，才能达到和128个神经元近似的效果。但是请注意左图中右上角的拟合效果——红色拟合线拐弯了！

## 样本数量的影响

样本数据量=500,1000，设置loop=int(num_samples/2)。

|example=500,iteraion=1562|example=1000,iteration=983|
|---|---|
|<img src=".\Images\8\r128-500.png">|<img src=".\Images\8\r128-1000.png">|<img src=".\Images\8\128-500-1000-010.png" width="600">

结论：样本数据量小，需要更多次的迭代。

## 学习率的影响

学习率=0.05,0.1，设置learning_rate=0.05。

|learning_rate=0.05,iteraion=6513|learning_rate=0.1,iteration=983|
|---|---|
|<img src=".\Images\8\r128-1000-005.png">|<img src=".\Images\8\r128-1000.png">|

结论：学习率太小，需要更多的迭代次数才能到达近似的效果。但右上角的拟合效果也不错。

铁柱：这个试例里能调的参数也就这么多了，更多的参数调整方法咱们可以在以后的章节中来学习体会。

木头：老师，那两个能拐弯儿的拟合是怎么回事儿？是否是过拟合了呢？

铁柱：哦！这种情况不好说，因为是末端了，如果训练数据能再向右扩充一些，比如到1.2，那么这个拐弯儿是正确的。实际上对于我们的标准结果（example=1000, neuron=128, learning_rate=0.1, iteraion=983），如果迭代次数再多些，也可能出现拐弯儿的情况。你可以下去试一下。

木头：好呀好呀！其实在上面的例子中，学习率是0.05时，迭代了6513次，最上端已经拐弯儿了。中间层神经元是96时，也有拐弯儿的迹象了。
