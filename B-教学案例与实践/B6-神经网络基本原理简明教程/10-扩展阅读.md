Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

（未完待续）

# 再谈训练集，验证集，测试集

木头：老师，第就章时您说过测试集的事儿，当时没什么概念，这次果然就用上了。这三个的英文是Training Set, Validation Set, Test Set，对吧？

铁柱：对！其中验证集Validation Set也可以叫开发集（Developer Set）。因为这次我们的手写字符的训练结果，无法用图形化的形式展示其准确程度，所以必须用独立的测试集来测试。注意，不能用测试集的数据去训练哦，否则就违背了神经网络训练的基本准则，自欺欺人了。

木头：啊！这么严重？

铁柱：在实际的生产环境中，Training Set用于训练模型，Validation Set用来统计评估指标，调节参数，选择算法，Test Set最后整体评估模型性能。

木头：那我们在这一章中，为什么没有Validation Set呢？

铁柱：因为我们的这个手写数字识别的问题，模型训练比较简单，一个两层的网络就足够了，参数也没有多少，所以中间的调整训练的过程并不复杂，所以没必要用验证集来帮忙，反而会增加工作复杂度。在一些复杂的模型中，模型可能只对训练数据有效，对验证数据就效果很差，你就根本没必要去用测试集做测试了。

木头：那具体如何使用Validation Set呢？

铁柱：在传统的机器学习中，比如一个SVM，我们经常用交叉验证(Cross Validation)的方法，比如把数据分成10份，V1-V10，其中V1-V9用来训练，V10用来验证。然后用V2-V10做训练，V1做验证......如此我们可以做10次训练和验证，大大增加了模型的可靠性。

木头：好办法！验证集也可以做训练，训练集数据也可以做验证，当样本很少时，这个很有用。那么深度学习中的用法是什么呢？

铁柱：比如在神经网络中，训练时到底迭代多少次停止呢？或者我们设置学习率为多少何时呢？或者用几个中间层，以及每个中间层用几个神经元呢？这些都可以用验证集来解决。在咱们前面的学习中，一般使用diff_loss < 1e-10做为迭代终止条件，虽然在绝大多数情况下可行，但不是绝对的，不能保证随机梯度下降算法陷入局部最优解。此时，我们可以用验证集来验证一下准确率，假设只有90%的的准确率，那可能确实是局部最优解。这样我们可以继续迭代，寻找全局最优解。

木头：那么这三者的比例关系如何调配呢？

铁柱：看下图吧。在传统的机器学习中，三者可以是6:2:2。在深度学习中，一般要求样本数据量很大，所以可以给训练集更多的数据，比如8:1:1。开发集，顾名思义，只给开发人员使用，他们看不到测试集。好比“公检法”三者的关系，公安局（训练集）负责抓坏人（我们抓到了一个模型！），检察院（验证集）负责诉讼（我们认为这个模型还不错！），法院（测试集）负责审判坏人（大锤子一敲：这个模型不好，重新去抓！），

<img src="./Images/10/dataset.jpg"/>

木头：（捂嘴狂笑）老师您以前是...公检法岗位的...？（继续松枝乱颤）

铁柱：笑什么笑！（恨不得真有个大锤子敲木头脑袋一下）讲真，我们还没有学习得到正则化一类的知识，那些参数都是靠验证集来确定的，并不是训练出来的。

木头：（一脸懵）肿么弄？

铁柱：正则化分为L1/L2两种，实际上是在误差函数后面增加一个项，以避免过度拟合。这个项里面有个参数，我们并不知道设置为多少合适，而是从0.01到1之间用验证集去试验训练效果。

木头：那如何知道是过拟合了呢？

铁柱：......啊，问题越来越深了，这个咱们后面遇到时再讲吧。

# 再谈数据归一化

木头：在第5/6两章中，我们已经用了数据归一化大法，否则有可能根本训练不出结果，如第5章中的房价数据，或者训练速度很慢，如第6章中的污染物分类。在这一章中，我看到我们使用了不一样的归一化数据处理方法，这是为什么呢？

下面这段代码是第5/6两章中使用的数据归一化方法：
```Python
def NormalizeData(X):
    X_NEW = np.zeros(X.shape)
    # get number of features
    n = X.shape[0]
    for i in range(n):
        x_row = X[i,:]
        x_max = np.max(x_row)
        x_min = np.min(x_row)
        if x_max != x_min:
            x_new = (x_row - x_min)/(x_max-x_min)
            X_NEW[i,:] = x_new
    return X_NEW
```

下面这段代码是第10章中使用的数据归一化方法：
```Python
def NormalizeData(X):
    X_NEW = np.zeros(X.shape)
    x_max = np.max(X)
    x_min = np.min(X)
    X_NEW = (X - x_min)/(x_max-x_min)
    return X_NEW
```

铁柱：眼神儿不错啊！很细心！你可以先把第5/6两章中的样本数据拿来咱们一起看看。

木头：好嘞！...（5分钟后）...我把样本数据列在下面了：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向|1|4|2|4|...|2|
|地理位置|3|2|6|3|...|3|
|面积|96|100|54|72|...|69|
|价格|434|500|321|482|...|410|

|样本序号|1|2|3|4|...|200|
|---|---|----|---|--|--|--|
|温度|0|13|2|24|...|8|
|湿度|55|14|46|58|...|78|
|污染物|0|2|1|2|...|0|

铁柱：好，你看看这两个样本与手写体识别的样本比较，有什么区别？

木头：（抓耳挠腮5分钟）... 怎么看啊？没法比较啊？

铁柱：（喝了口绿茶）提示一下，主要看数值方面。

木头：（喝了口二锅头）... 啊！有点儿明白了，这两个数据，样本的每个特征值的取值范围与其它特征值都不同，比如房屋朝向取值[1,4]，地理位置取值[2,6]，面积取值[50,140]。污染物的温度取值[0,40]，湿度取值[0,100]。

铁柱：嗯嗯！那么MNIST的数据呢？

木头：MNIST的数据全是[0,255]，我们是这样排列的：

|样本序号|1|2|3|4|...|60000|
|---|---|----|---|--|--|--|
|点1|0|0|0|0|...|0|
|点2|0|0|12|0|...|59|
|点3|0|0|56|253|...|98|
|点m|0|23|148|0|...|0|
|点784|0|0|0|0|...|0|

也就是说，数据虽然分成多个特征值（行），但是每个特征的取值范围实际上都是[0,255]。

铁柱：对！咱们在第5/6章中的归一化代码，输入是整个表格X，先得到X.shape[0]，也就是特征值的数量，然后逐行做归一化，也就是说房屋朝向和地理位置的归一化范围并不相同。而第10章中，所有数据全是[0,255]，如果逐行归一，很可能出现这种情况：

- 有一行的数据是：(2,23,148,...3,45)，最大值是148，最小值是2，那么就会把2归一为0，把148归一为1。
- 而在另外一行的数据可能是：(0,3,248,...13,4)，会把0归一为0，把248归一为1。

这样的话，这两行的数据就不可比了，也就是说，本来都是点阵数据，表示灰度的，却被人为地改变了数据的相对值。

木头：哦！对！把图像的二维点阵处理成一维的特征值，是为了计算方便，并不是要改变点阵本身的数值特性。但是，老师，如果不小心这样做了，会有什么问题吗？

铁柱：对于这个手写识别的例子来说，没有多大影响，因为每个特征之间没什么联系，值的相对变化不会影响训练结果。对于别的例子就不好说了，以后咱们遇到时再分析。

# 再谈权重矩阵初始化

铁柱：木头脑袋，你有没有注意到咱们这次的InitialParameters()函数有了变化？

木头：啊？（一头雾水）等我看看...（2分钟后）哦！增加了一个对于W的初始化方法flag=2：

```Python
def InitialParameters(num_input, num_hidden, num_output, flag):
    if flag == 0:
        # zero
        W1 = np.zeros((num_hidden, num_input))
        W2 = np.zeros((num_output, num_hidden))
    elif flag == 1:
        # normalize
        W1 = np.random.normal(size=(num_hidden, num_input))
        W2 = np.random.normal(size=(num_output, num_hidden))
    elif flag == 2:
        #
        W1=np.random.uniform(-np.sqrt(6)/np.sqrt(num_input+num_hidden),np.sqrt(6)/np.sqrt(num_hidden+num_input),size=(num_hidden,num_input))
        W2=np.random.uniform(-np.sqrt(6)/np.sqrt(num_output+num_hidden),np.sqrt(6)/np.sqrt(num_output+num_hidden),size=(num_output,num_hidden))

    B1 = np.zeros((num_hidden, 1))
    B2 = np.zeros((num_output, 1))
    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param
```

铁柱：是的，这种初始化方法叫Xavier权重初始化。flag=1时，是随机初始化，flag=0时就是全0初始化。前面我们说过，全0初始化时，根本训练不出东西来。我们比较一下1和2时的区别：


|flag = 1, 随机初始化|flag = 2, Xavier初始化|
|---|---|
|rate=9040 / 10000 = 0.9040%|rate=9407 / 10000 = 0.9407%|
|<img src="./Images/10/Init-Random.png">|<img src="./Images/10/Init-Xavier.png">|


非常明显，随机初始化的Loss一开始就很大，下降的速度也不理想，在0.3左右徘徊，最后的准确率是90.4%。而Xavier初始化的Loss一开始就小，下降的就快，很快小于0.2了，最后的准确率是94.07%。

木头：那是为什么呢？

铁柱：这个原理比较复杂，今天老师肚子疼，咱们先记住结论就好，Xavier初始化的公式是：

$$
W = Uniform(-\sqrt{\frac{6}{n_j+n_{j+1}}}, \sqrt{\frac{6}{n_j+n_{j+1}}})
$$

其中，$n_j$表示当前层的节点数，$n_{j+1}$表示下一层的节点数。对于W1来说，当前层是num_input, 下一层是num_hidden。对于W2来说，当前层是num_hidden，下一层是num_output。Uniform表示均匀分布。

木头：好滴~，记住结论也很重要。我现在体会到深度学习的很多东西本来就是以深厚的数学为基础推导出来的，数学家为我们打好了基础，我们先站在巨人的肩膀上好了。

铁柱：你回去后可以看看相关资料。

木头：好哒好哒......诶哟，老师我也肚子疼！您借我点儿纸......（抱头鼠窜）