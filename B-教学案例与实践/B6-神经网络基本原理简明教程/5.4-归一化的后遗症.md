Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 归一化的后遗症

## 对比数据

第二天......

木头：W，B的值都算出来了，说明网络收敛了，这是成功的第一步。再看最后的预测值，93平米的房子需要36839万元！
难道是传说中的黄金屋？这不科学！

铁柱：你在昨天的表的基础上，计算一下W的变化率，再和特征缩放值去比较一下。

木头：好哒！

|结果|W1|W2|W3|B|
|---|---|---|---|---|
|正规方程|2|-10|5|110|
|神经网络|5.99|-40.00|394.99|292.00
|倍差|5.99/2=3|-40/-10=4|394.99/5=79|292/110=2.65|


铁柱：你记得咱们给样本数据做了归一化吧？那么把归一化前后的数据也列在这里吧。

|特征|朝向|位置|面积|
|----|----|---|---|
|最小值|1|2|40|
|最大值|4|6|119|
|范围|4-1=3|6-2=4|119-40=79|

木头：咦！通过对比我发现，第一张表最后一行的数据，和第二张表最后一行的数据，有惊人的相似之处！这是为什么呢？

## 还原真实的W,B值

铁柱：我们唯一修改的地方，就是样本数据特征值的归一化，我们并没有修改标签值！

木头：哦！明白了......没明白......好像有点儿明白了，只能看出和样本的缩放有关系，而且缩放倍数一样。可是B值怎么解释呢？

铁柱：假设在归一化之前，真实的样本值是$X$，真实的权重值是$W$；在归一化之后，样本值变成了$X'$，训练出来的权重值是$W'$：

$$
Y = W \cdot X +B \tag{Y是标签值}
$$

$$
Z = W' \cdot X' +B' \tag{Z是预测值}
$$

由于训练时标签值（房价）并没有做归一化，意味着我们是用真实的房价做的训练，所以预测值和标签值应该相等，所以：
$$
Y == Z $$$$
W \cdot X +B = W' \cdot X'+B' \tag{1}
$$

归一化的公式是：
$$
X' = {X - X_{min} \over X_{max}-X_{min}} \tag{2}
$$

把公式2代入公式1：

$$
W \cdot X +B = W' \cdot {X - X_{min} \over X_{max}-X_{min}} + B'$$$$
=W' \cdot {X \over X_{max}-X_{min}} - W' \cdot {X_{min} \over X_{max}-X_{min}} + B'$$$$
={W'X \over X_{range}} - {W'X_{min} \over X_{range}}+B'$$
第二项是个常数，即：
$$
W \cdot X +B = {W' \over X_{range}} \cdot X - {W'X_{min} \over X_{range}} + B' \tag{3}
$$
如果想让公式3等式成立，则变量项和常数项分别相等，即：
$$
W \cdot X = {W' \over X_{range}} \cdot X \tag{4}$$$$ 
B = - {W'X_{min} \over X_{range}} + B' \tag{5}
$$
从公式4，两边除以X，可以得到：
$$
W = {W' \over X_{range}} \tag{6}
$$

木头：我来算一下......

$$
W1 = {W1' \over X1_{range}} = {5.99 \over (4-1)} \approx 2$$$$
W2 = {W2' \over X2_{range}} = {-40 \over (6-2)} \approx -10$$$$
W3 = {W3' \over X3_{range}} = {394.99 \over (119-40)} \approx 5$$$$
B = B' - {W1'X1_{min} \over X1_{range}} - {W2'X2_{min} \over X2_{range}} - {W3'X3_{min} \over X3_{range}}$$$$
= 292 - {5.99 \times 1 \over 4-1}-{-40 \times 2 \over 6-2}-{394.99 \times 40 \over 119-40}$$$$
= 292-2+20-200=110
$$

这个和正规方程的解能够完全匹配！

## 变成代码
```Python
# get real weights
def DeNormalizeWeights(X_range, x_min, w, b):
    n = w.shape[1]
    W_real = np.zeros((n,))
    for i in range(n):
        W_real[i] = w[0,i] / X_range[0,i]

    B_real = b
    for i in range(n):
        tmp = w[0,i] * x_min[0,i] / X_range[0,i]
        B_real = B_real - tmp
    return W_real, B_real
```

X_range是我们在做归一化时保留下来的样本的三个特征向量的取值范围，x_min是三个特征向量的最小值。

在主程序最后两行加一下这个逻辑，再次运行：

```Python
W_real, B_real = DeNormalizeWeights(X_range, X_min, w, b)
print("W_real=", W_real)
print("B_real=", B_real)

```
运行结果如下：
```
w= [[  5.9964012  -40.00429782 394.98808748]]
b= [[292.0105229]]
epoch=14, iteration=99, loss=0.000008
W_real= [  1.9988004  -10.00107445   4.99984921]
B_real= [[110.01990306]]
```

木头：老师，这次我们算是成功了吗？

铁柱：对于简单的线性问题来说，这么做可以。但是如果遇到非线性问题，或者深层网络，这么做就不行了。咱们下回分解！

代码位置：ch05/level4
