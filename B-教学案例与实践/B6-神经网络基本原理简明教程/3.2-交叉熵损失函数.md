Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 交叉熵函数

**交叉熵函数常用于逻辑回归(logistic regression)，也就是分类(classification)。**

### 公式

$$
Loss = -\frac{1}{m} \sum_i^m [y_i log(a_i) + (1-y_i)log(1-a_i)],  y_i \in \{0,1\}
$$
单个样本：
$$
loss = -[y \cdot lna+(1-y) \cdot ln(1-a)]
$$
### 工作原理


当y=1时，即标签值是1，是个正例：

$$
loss = -lna
$$

此时，损失值与预测值的关系是：

<img src="./Images/3/crossentropy1.png"/>

横坐标是预测输出，纵坐标是损失函数值。y=1意味着当前样本标签值是1，当预测输出越接近1时，Loss值越小，训练结果越准确。当预测输出越接近0时，Loss值越大，训练结果越糟糕。

当y=0时，即标签值是0，是个反例：
$$
loss = -ln(1-a)
$$

此时，损失值与预测值的关系是：

<img src="./Images/3/crossentropy2.png"/>

横坐标是预测输出，纵坐标是损失函数值。y=0意味着当前样本标签值是0，当预测输出越接近0时，Loss值越小，训练结果越准确。当预测输出越接近1时，Loss值越大，训练结果越糟糕。

### 推导过程

标签值为1的概率：

$$a=P(y=1|x)$$

很明显，当前样本标签为 0 的概率就可以表达成：

$$1−a=P(y=0|x)$$

从极大似然性的角度出发，把上面两种情况整合到一起：

$$
P(y|x)=a^y⋅(1−a)^{1−y}
$$

当真实样本标签 y = 0 时，上面式子第一项就为 1，概率等式转化为：

$$P(y=0|x)=1−a$$

当真实样本标签 y = 1 时，上面式子第二项就为 1，概率等式转化为：

$$
P(y=1|x)=a
$$

两种情况下概率表达式跟之前的完全一致，只不过我们把两种情况整合在一起了。

我们希望的是概率 P(y|x) 越大越好。我们对 P(y|x) 引入 log 函数，因为 log 运算并不会影响函数本身的单调性。则有：

$$
log P(y|x)=log(a^y⋅(1−a)^{1−y})=y \cdot loga+(1−y) \cdot log(1−a)
$$

我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为：

$$
loss=-[y \cdot loga+(1−y) \cdot log(1−a)]
$$

### 交叉熵函数的使用

分类问题中，包含二分类和多分类两种情况。


#### 二分类

最后输出层使用Sigmoid激活函数，用如下交叉熵损失函数形式：

$$
loss = -[y_i \ln a_i + (1-y_i) \ln (1-a_i)]
$$


#### 多分类（类别大于等于3）

多分类：最后输出层使用Softmax激活函数，用如下交叉熵损失函数形式：
$$
loss = -y_i \ln a_i
$$

假设一共有3个类别，期望输出是y=[1.0, 0.0, 0.0]，即标签值表明当前样本的实际输出应该是第一类。

而实际的训练中，假设测输出是a1=[0.5, 0.2, 0.3]，则损失函数计算如下：

$$Loss1 = -\sum y_i \ln a1_i $$$$
-(1 \times \ln 0.5 + 0 \times \ln 0.2 + 0 \times \ln 0.3)=0.693
$$

假设测输出是a2=[0.8, 0.1, 0.1]，则损失函数计算如下：

$$Loss2 = -\sum y_i \ln a2_i $$$$
-(1 \times \ln 0.8 + 0 \times \ln 0.1 + 0 \times \ln 0.1)=0.223
$$

可以看到a2的损失函数值较小，因为0.8比0.5更接近1.0。


## 均方差和交叉熵的比较

主要是在经过Sigmoid之后的输出值，在求导后，Sigmoid本身的导数是否存在的问题。
在神经网络中，假设在损失函数之前的激活函数是Sigmoid函数：

$$
a=\frac{1}{1+e^{-z}}
$$

<img src="./Images/3/grad_MSE.png"/>

以均方差函数为损失函数，对z求导：

$$
\frac{\partial{Loss}}{\partial{z}}=\frac{\partial{Loss}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{z}}$$$$
=(a-y) \cdot a(1-a)
$$

其中，a(1-a)项是Sigmoid函数的导数，注意上图中红色的导数曲线，其最大值只有0.25，意味着从最后端传回来的误差值，最多只能以0.25倍的比例向前传，这就给反向传播的效果打了折扣。

以交叉熵函数为损失函数，对z求导：

$$
\frac{\partial{Loss}}{\partial{z}}=\frac{\partial{Loss}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{z}}$$$$
= \frac{a-y}{a(1-a)} \cdot a(1-a) = a-y
$$

比较上述两个求导结果可以得出结论：
1. **均方差函数适用于回归（函数拟合）**。当均方差函数前面有Sigmoid激活函数时，当误差很大时，比如大于5时，a(1-a)项的数值会非常小，造成了反向传播的力度很小。所以我们在做线性回归（函数拟合）时，不用Sigmoid函数做激活函数，而是直接用最后一层的线性输出作为Loss函数的输入。

2. **交叉熵函数一般用于分类**。因为交叉熵函数前面连接了激活函数时，激活函数的导数的不良性质不会影响反向传播的力度，从公式看就是只有a-y这样一项。
