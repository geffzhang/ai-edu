Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 知识点
- 非线性逻辑回归
- 初始化
- 数据归一化
- 双层神经网络
- 输出层激活函数的使用

# 提出问题

果农最头疼的问题就是果树害虫。以苹果树为例，其最短为15年的寿命，一直会受到虫害，比如苹果红蜘蛛。下面的数据是苹果树在10年生长期内每年四个季度中，出现虫害的可能性。

|样本序号|1|2|3|4|...|200|
|---|---|---|---|--|--|---|
|生长年分[0,10]|0.58|4.46|8.61|2.44|...|3.13|
|季节[0,3]|1.2|0.6|2.5|0.8|...|1.2|
|害虫出现可能性[0,1]|0|0|1|1|...|0|

数据中，季节0~3表示冬春夏秋四季，小数点可以理解为一个季节被整分为10份，小数点后面1位数字表示9天。

### 问题：一颗果树的生长年份=2.2年，季节=春天，预测出该时间点出现害虫的可能性是多少？

还好这个数据只有两个特征，所以我们可以用可视化的方法展示，如下图：

<img src='./Images/9/Sample.png'/>

红色点表示害虫出现可能性为0，蓝色点表示可能性为1。

从图中可以明显看出，这不是线性可分问题。单层神经网络只能做线性分类，如果想做非线性分类，需要至少两层神经网络来完成。



# 定义神经网络结构

<img src='./Images/9/NN.jpg'/>

矩阵运算过程：
1. $W1(10,2) * X(2,1) + B1(10,1) => Z1(10,1)$
2. $Sigmoid(Z1) => A1(10,1)$
3. $W2(1,10) * A1(10,1) + B2(1,1) => Z2(1,1)$
4. $Sigmoid(Z2) => A2(1,1)$

## 输入层

$$
X = \begin{pmatrix}
X_1 & X_2 \dots X_{200} 
\end{pmatrix}
= \begin{pmatrix}
x_{1,1} & x_{2,1} \dots x_{200,1} \\
x_{1,2} & x_{2,2} \dots x_{200,2}
\end{pmatrix}
$$

我们的约定是每行是一个样本，每列一个样本的所有特征，这里是2个特征，第一个是生长年份，第二个是季节。

## 中间层（隐藏层）

定义10个神经元，当然在代码里要把这个数字写成参数可调的。

$$
Z1 = \begin{pmatrix}
Z1_1 \\ 
Z1_2 \\ 
\dots \\
Z1_{10} \end{pmatrix},
A1 = \begin{pmatrix}
A1_1 \\ 
A1_2 \\ 
\dots \\
A1_{10} \end{pmatrix},
$$

其中，$Z1=W1*X+B1，A1=Sigmoid(Z1)$，大1表示第一层神经网络。

## 权重矩阵W1/B1
$$
W1=\begin{pmatrix}
w_{1,1} & w_{1,2} \\
w_{2,1} & w_{2,2} \\
\dots \\
w_{10,1} & w_{10,2}
\end{pmatrix}
$$

B1的尺寸是32x1，行数永远和W一样，列数永远是1。

$$
B1=\begin{pmatrix}
b_1 \\
b_2 \\
\dots \\
b_{10}
\end{pmatrix}
$$

## 输出层

输出层1个神经元，再加上一个Sigmoid激活：

$$
Z2 = \begin{pmatrix}z_1 \end{pmatrix},
A2 = \begin{pmatrix} a_1 \end{pmatrix}
$$

其中，$Z2=W2*A1+B2，A2=Softmax(Z2)$，大2表示第二层神经网络。

## 权重矩阵W2/B2

$$
W2=\begin{pmatrix}
w_{1,1} & w_{1,2} \dots w_{1,32}\\
w_{2,1} & w_{2,2} \dots w_{2,32} \\
\dots \\
w_{10,1} & w_{10,2} \dots w_{10,32}
\end{pmatrix}
$$

B2的尺寸是10x1，行数永远和W一样，列数永远是1。

$$
B2=\begin{pmatrix}
b_1 \\
b_2 \\
\dots \\
b_{10}
\end{pmatrix}
$$

# 下载训练数据

[点击下载训练数据](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/TreeWormXData.dat)

[点击下载标签数据](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/TreeWormYData.dat)


# 读取文件数据
```Python
def LoadData():
    Xfile = Path("TreeXData.dat")
    Yfile = Path("TreeYData.dat")
    if Xfile.exists() & Yfile.exists():
        XData = np.load(Xfile)
        YData = np.load(Yfile)
        return XData,YData
    
    return None,None
```

# 数据归一化
```Python
def NormalizeByData(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    x_range = np.zeros((1,n))
    x_min = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        max_value = np.max(x)
        min_value = np.min(x)
        x_min[0,i] = min_value
        x_range[0,i] = max_value - min_value
        x_new = (x - x_min[0,i])/(x_range[0,i])
        X_new[i,:] = x_new
    return X_new, x_range, x_min

# normalize data by specified range and min_value
def NormalizeByRange(X, x_range, x_min):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    for i in range(n):
        x = X[i,:]
        x_new = (x-x_min[0,i])/x_range[0,i]
        X_new[i,:] = x_new
    return X_new
```


# 前向计算
```Python
def ForwardCalculation(X, dict_Param):
    W1 = dict_Param["W1"]
    B1 = dict_Param["B1"]
    W2 = dict_Param["W2"]
    B2 = dict_Param["B2"]
    
    Z1 = np.dot(W1,X)+B1
    A1 = Sigmoid(Z1)
    #A1 = np.tanh(Z1)

    Z2=np.dot(W2,A1)+B2
    A2=Sigmoid(Z2)
    
    dict_Cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    return A2, dict_Cache
```

# 定义代价函数
```Python
# cross entropy: -(Y*lnA+(1-Y)ln(1-A))
def CalculateLoss(dict_Param, X, Y, count):
    A2, dict_Cache = ForwardCalculation(X, dict_Param)
    p = Y * np.log(A2) + (1-Y) * np.log(1-A2)
    Loss = -np.sum(p) / count
    return Loss
```

# 反向传播
```Python
def BackPropagation(dict_Param,cache,X,Y):
    W1=dict_Param["W1"]
    W2=dict_Param["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]
    Z1=cache["Z1"]

    dZ2= A2 - Y
    dW2 = np.dot(dZ2, A1.T)
    dB2 = np.sum(dZ2, axis=1, keepdims=True)

    dLoss_A1 = np.dot(W2.T, dZ2)
    dA1_Z1 = A1 * (1 - A1)     # sigmoid
    #dA1_Z1 = 1-np.power(A1,2)   # tanh
    dZ1 = dLoss_A1 * dA1_Z1
    
    dW1 = np.dot(dZ1, X.T)
    dB1 = np.sum(dZ1, axis=1, keepdims=True)

    dict_Grads = {"dW1": dW1, "dB1": dB1, "dW2": dW2, "dB2": dB2}
    return dict_Grads

def UpdateParam(dict_Param, dict_Grads, learning_rate):
    W1 = dict_Param["W1"]
    B1 = dict_Param["B1"]
    W2 = dict_Param["W2"]
    B2 = dict_Param["B2"]

    dW1 = dict_Grads["dW1"]
    dB1 = dict_Grads["dB1"]
    dW2 = dict_Grads["dW2"]
    dB2 = dict_Grads["dB2"]

    W1 = W1 - learning_rate * dW1
    B1 = B1 - learning_rate * dB1
    W2 = W2 - learning_rate * dW2
    B2 = B2 - learning_rate * dB2

    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param
```

# 帮助函数
```Python
def Sigmoid(x):
    s=1/(1+np.exp(-x))
    return s
```

# 初始化
```Python
def InitialParameters(num_input, num_hidden, num_output, flag):
    if flag == 0:
        # zero
        W1 = np.zeros((num_hidden, num_input))
        W2 = np.zeros((num_output, num_hidden))
    elif flag == 1:
        # normalize
        W1 = np.random.normal(size=(num_hidden, num_input))
        W2 = np.random.normal(size=(num_output, num_hidden))

    B1 = np.zeros((num_hidden, 1))
    B2 = np.zeros((num_output, 1))
    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param
```

# 预测
```Python
def Predicate(dict_Param, xt, x_range, x_min):
    xt_new = NormalizeByRange(xt, x_range, x_min)
    a2, cache = ForwardCalculation(xt_new, dict_Param)
    return a2
```

# 程序主循环
```Python
print("Loading...")
learning_rate = 0.1
num_hidden = 10
num_output = 1
raw_data,Y = LoadData()
X,X_range,X_min = NormalizeByData(raw_data)

num_images = X.shape[1]
num_input = X.shape[0]
max_iteration = 1000

dict_Param = InitialParameters(num_input, num_hidden, num_output, 2)
prev_loss = 0
diff_loss = 10
eps = 1e-10
print("Training...")
for iteration in range(max_iteration):
    for item in range(num_images):
        x = X[:,item].reshape(num_input,1)
        y = Y[item]
        A2, dict_Cache = ForwardCalculation(x, dict_Param)
        dict_Grads = BackPropagation(dict_Param, dict_Cache, x, y)
        dict_Param = UpdateParam(dict_Param, dict_Grads, learning_rate)
        Loss = CalculateLoss(dict_Param, X, Y, num_images)
        diff_loss = np.abs(Loss-prev_loss)
        prev_loss = Loss
        if diff_loss < eps:
            break
    print(iteration, Loss)
    if diff_loss < eps:
        break
 
print("complete")
print("W1:", dict_Param["W1"])
print("B1:", dict_Param["B1"])
print("W2:", dict_Param["W2"])
print("B2:", dict_Param["B2"])

print("testing...")
 
ShowResult(X, Y)

# 2.2年，1=春天
x_test = np.array([2.2, 1.0]).reshape(2,1)
result = Predicate(dict_Param, x_test, X_range, X_min)
print(result)
```

# 运行结果

下图展示了运行结果，图中的绿色线就是我们的神经网络计算出来的分类边界线。

```
356 0.1828751346397973
357 0.18234516967539616
358 0.1829374974523464
complete
W1: [[-11.17500351  -0.61160314]
 [  1.44396499  -6.73766435]
 [ -2.02466011   7.93755681]
 [ 16.95246547   0.37914396]
 [  0.40255466  -0.89695915]
 [ -0.98761665  -0.61616035]
 [ -2.1285229    8.20946753]
 [ -0.7829629    1.75997918]
 [ -0.18069186   0.91821172]
 [ -0.40097791  -1.15791768]]
B1: [[  4.29433563]
 [  3.74956472]
 [ -4.60340221]
 [-10.13340555]
 [ -1.70041155]
 [ -1.41553215]
 [ -4.72695367]
 [ -1.50595271]
 [ -1.65535398]
 [ -1.52833061]]
W2: [[ 6.62704299  1.30607306 -5.06544219 12.8452811  -0.32550167  0.29022754
  -5.26221025 -1.15785966 -0.94631672  0.28244829]]
B2: [[-4.45352878]]
testing...
[[0.71079945]]
```
结果是有虫害的概率为0.71，所以，农民伯伯们，打药吧！

