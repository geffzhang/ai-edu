Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

## sigmoid函数

公式：$f(z) = \frac{1}{1 + e^{-z}}$

反向传播： $f^{'}(z) = f(z) * (1 - f(z))$，推导过程请参看[数学导数公式](https://www.cnblogs.com/ms-uap/p/9957269.html)。

<img src=".\Images\7\sigmoid.png">

从函数图像来看，sigmoid函数的作用是将输入限制到(0, 1)这个区间范围内，这种输出在0~1之间的函数可以用来模拟一些概率分布的情况。他还是一个连续函数，导数简单易求。  

用mnist数据的例子来通俗的解释一下：

<img src=".\Images\7\mnist_2layer.png">

形象化的说，每一个隐藏层神经元代表了对某个笔画的感知，也就是说可能第一个神经元代表是否从图中检测到有一条像1一样的竖线存在，第二个神经元代表是否有一小段曲线的存在。但是在实际传播中，怎么表示是不是有这样一条直线或者这样一段曲线存在呢？在生活中，我们经常听到这样的对白“你觉得这件事情成功概率有多大？”“我有六成把握能成功”。sigmoid函数在这里就起到了如何把一个数值转化成一个通俗意义上的把握的表示。值越大，那么这个神经元对于这张图里有这样一条线段的把握就越大，经过sigmoid函数之后的结果就越接近100%，也就是1这样一个值，表现在图里，也就是这个神经元越兴奋（亮）。

但是这个样子的激活函数有什么问题呢？

从梯度图像中可以看到，sigmoid的梯度在两端都会接近于0，根据链式法则，把其他项作为$\alpha$,那么梯度传递函数是$\alpha*\sigma'(x)$，而$\sigma'(x)$这时是零，也就是说整体的梯度是零。这也就很容易出现梯度消失的问题，并且这个问题可能导致网络收敛速度比较慢，比如采取MSE作为损失函数算法时。

给个纯粹数学的例子吧，假定我们的学习速率是0.2，sigmoid函数值是0.9，如果我们想把这个函数的值降到0.5，需要经过多少步呢？

我们先来做公式推导，
第一步，求出当前输入的值

$$\frac{1}{1 + e^{-x}} = 0.9$$
$$e^{-x} = \frac{1}{9}$$
$$x = ln{9}$$

第二步，求出当前梯度

$$grad = f(x)\times(1 - f(x)) = 0.9 \times 0.1= 0.09$$

第三步，根据梯度更新当前输入值

$$x_{new} = x - \eta \times grad = ln{9} - 0.2 \times 0.09 = ln(9) - 0.018$$

第四步，判断当前函数值是否接近0.5

$$\frac{1}{1 + e^{-x_{new}}} = 0.898368$$

第五步，重复步骤2-3直到当前函数值接近0.5

说得如果不够直观，那我们来看看图，

<img src=".\Images\7\decay_with_relu.png">

上半部分那条五彩斑斓的曲线就是迭代更新的过程了，一共迭代了多少次呢？根据程序统计，sigmoid迭代了67次才从0.9衰减到了接近0.5的水准。有同学可能会说了，才67次嘛，这个次数也不是很多啊！确实，从1层来看，这个速度还是可以接受的，但是神经网络只有这一层吗？多层叠加之后的sigmoid函数，因为反向传播的链式法则，两层的梯度相乘，每次更新的步长更小，需要的次数更多，也就是速度更加慢。如果还是没有反应过来的同学呢，可以先向下看relu函数的收敛速度。

此外，如果输入数据是(-1, 1)范围内的均匀分布的数据会导致什么样的结果呢？经过sigmoid函数处理之后这些数据的均值就从0变到了0.5，导致了均值的漂移，在很多应用中，这个性质是不好的。

代码思路：  
放到代码中应该怎么实现呢？首先，对于一个输入进sigmoid函数的向量来说，函数的输出和反向传播时的导数是和输入的具体数值有关系的，那么为了节省计算量，可以生成一个和输入向量同尺寸的mask，用于记录前向和反向传播的结果，具体代码来说，就是：

示例代码：

```python
class Csigmoid(object):
    def __init__(self, inputSize):
        self.shape = inputSize

    def forward(self, image):
        # 记录前向传播结果
        self.mask = 1 / (1 + np.exp(-1 * image))
        return self.mask

    def gradient(self, preError):
        # 生成反向传播对应位置的梯度
        self.mask = np.multiply(self.mask, 1 - self.mask)
        return np.multiply(preError, self.mask)
```

不理解为啥又有前向传播又有梯度计算的小伙伴请戳[这里](https://www.cnblogs.com/ms-uap/p/9928254.html)

## tanh函数

形式：  
$f(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$  
$f(z) = 2*sigmoid(2*z) - 1$

反向传播:
$f^{'}(z) = (1 + f(z)) * (1 - f(z))$

<img src=".\Images\7\tanh.png">

无论从理论公式还是函数图像，这个函数都是一个和sigmoid非常相像的激活函数，他们的性质也确实如此。但是比起sigmoid，tanh减少了一个缺点，就是他本身是零均值的，也就是说，在传递过程中，输入数据的均值并不会发生改变，这就使他在很多应用中能表现出比sigmoid优异一些的效果。

代码思路：  
这么相似的函数没有相似的代码说不过去呀！比起sigmoid的代码，实现tanh只需要改变几个微小的地方就可以了，话不多说，直接上代码吧：

示例代码:

```python
class Ctanh(object):
    def __init__(self, inputSize):
        self.shape = inputSize

    def forward(self, image):
        # 记录前向传播结果
        self.mask = 2 / (1 + np.exp(-2 * image)) - 1
        return self.mask

    def gradient(self, preError):
        # 生成反向传播对应位置的梯度
        self.mask = np.multiply(1 + self.mask, 1 - self.mask)
        return np.multiply(preError, self.mask)
```

