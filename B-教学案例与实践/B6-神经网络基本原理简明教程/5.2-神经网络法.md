Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。这个一层的神经网络没有中间层，只有输入项和输出层（输入项不算做一层），而且只有一个神经元，并且神经元有一个线性输出，不经过激活函数处理。亦即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

<img src=".\Images\5\setup.jpg" width="600">

矩阵运算过程：$W(1,3) * X(3,1) + B(1,1) => Z(1,1)$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(1,3)表示W是一个1行3列的矩阵。

## 输入层

我们先看一下样本数据的样子：


|样本序号|1|2|3|...|1000|
|---|---|----|---|--|--|
|朝向（东南西北）|2|2|3|...|3|
|地理位置（几环）|4|6|3|...|3|
|居住面积（平米）|79|116|109|...|98|
|整套价格（万元）|469|631|323|...|576|


单独看一个样本是这样的：

$$
x_1 =
\begin{pmatrix}
x_{1,1} \\
x_{1,2} \\
x_{1,3}
\end{pmatrix} = 
\begin{pmatrix}
1 \\
3 \\
96
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 434 \end{pmatrix}
$$


一共有1000个样本，每个样本3个特征值，X就是一个$3 \times 1000$的矩阵，模样是酱紫的：

$$
X = \\
\begin{pmatrix} 
X_1 & X_2 \dots X_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{2,1} & \dots & x_{1000,1} \\
x_{1,2} & x_{2,2} & \dots & x_{1000,2} \\
x_{1,3} & x_{2,3} & \dots & x_{1000,3}
\end{pmatrix} = 
\begin{pmatrix}
2 & 2 & 3 & \dots & 3 \\
4 & 6 & 3 & \dots & 3 \\
79 & 116 & 109 & \dots & 98
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y_1 & y_2 & \dots & y_m \\
\end{pmatrix}=
\begin{pmatrix}
469 & 631 & 323 & \dots & 576 \\
\end{pmatrix}
$$


$X_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

## 权重W和B

木头：老师，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？那样好像更符合思维习惯？

铁柱：确实是！但是在实际的矩阵运算时，由于是$Z=W \cdot X+B$，W在前面，X在后面，所以必须是这个样子的：

$$
\begin{pmatrix}
w_1 & w_2 & w_3
\end{pmatrix}
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix}=
w_1 \cdot x_1+w_2 \cdot x_2+w_3 \cdot x_3
$$

假设每个样本x有n个特征向量，上式中的W就是一个$1 \times n$的向量，让每个w都对应一个x：
$$
\begin{pmatrix}w_1 & w_2 \dots w_n\end{pmatrix}
$$

B是个单值，因为只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

## 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。由于是线性的，所以没有用激活函数。

对于拟合，可以想象成用一支笔在一堆点中画一条直线或者曲线，而那一个神经元就是这支笔。如果有多个神经元，可以画出多条线来，就不是拟合了，而是分类。


# 代码

代码位置：ch05目录，level2-NeuralNetwork. py

# 运行

怀着期待的心情用颤抖的右手按下了运行键......but......what happened?

```
epoch=0
0 0 55883834476.133575 [[ 101.0507623   201.66401831 3960.19459875]] [[50.13519931]]
0 1 2.075927906874647e+16 [[  -61796.96353768  -123594.36458166 -2410062.36310063]] [[-30898.87195068]]
0 2 3.7752686053459638e+22 [[5.60309885e+07 1.68154762e+08 3.25097149e+09]] [[28015493.83866825]]
0 3 5.332628913256499e+28 [[-1.06460908e+11 -1.06348785e+11 -3.86686449e+12]] [[-3.5477631e+10]]
......
0 110 inf [[5.83856788e+303 1.17197663e+304 3.22400448e+305]] [[5.86699589e+303]]
0 111 inf [[-1.00706180e+307 -1.00647368e+307 -inf]] [[-3.35295186e+306]]
0 112 nan [[inf inf nan]] [[inf]]
0 113 nan [[nan nan nan]] [[nan]]
0 114 nan [[nan nan nan]] [[nan]]
......
```

怎么会overflow呢？于是右手的颤抖没有停止，左手也开始颤抖了。

数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。我们再看看损失函数历史记录：

<img src=".\Images\5\wrong_loss.png">

损失函数值随着迭代次数快速上升，说明训练没有收敛，而是发散了。

木头：老师，这......这不是我干的！

铁柱：也不是我干的，别慌，咱们在下一节内容中解决这个问题！

木头：好吧好吧！

代码位置：ch05/level2