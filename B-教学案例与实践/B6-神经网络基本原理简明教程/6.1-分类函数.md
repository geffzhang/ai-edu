Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

回归和分类，是神经网络擅长的两种工作。回归就是拟合，分类就是把样本通过训练分成指定的几个类别。老子说：一生二，二生三，三生万物。其中的“一”就是回归，“二”就是二分类，“三”就是多分类。二分类和多分类在实际使用种略有不同。


# 二分类
在输出层使用单个神经元输出，使用Sigmoid激活函数和下面这种交叉熵损失函数：
$$
Z = W * X + B \tag{矩阵计算}
$$

$$
A=Sigmoid(Z) \tag{Logistic激活函数用于二分类}
$$

$$
Loss = -[Y * lnA+(1-Y) * ln(1-A)] \tag{二分类交叉熵函数}
$$

|<img src=".\Images\1\NeuranCell.png" width="400"/>|<img src=".\Images\1\activation.png" width="400"/>|
|---|---|

分类的方式是，可以指定当A > 0.5时是正例，A <= 0.5时就是反例。或者根据实际情况指定别的阈值比如0.3，0.8等等。

此时反向传播公式推导结果是：

$$
\frac{\partial{Loss}}{\partial{A}}=-[\frac{Y}{A}+\frac{1-Y}{1-A}*(-1)]=\frac{A-Y}{A(1-A)} \tag{交叉熵函数求导}
$$
$$
\frac{\partial{A}}{\partial{Z}}=A(1-A) \tag{Sigmoid激活函数求导}
$$
$$
\frac{\partial{Z}}{\partial{W}}=X^T , \frac{\partial{Z}}{\partial{B}}=1 \tag{矩阵运算求导}
$$

所以：

$$
\frac{\partial{Loss}}{\partial{W}} = \frac{\partial{Loss}}{\partial{A}} * \frac{\partial{A}}{\partial{Z}} * \frac{\partial{Z}}{\partial{W}} \\
=\frac{A-Y}{A(1-A)} * A(1-A) * X^T \\
=(A-Y) * X^T \tag{W的梯度}
$$

$$
\frac{\partial{Loss}}{\partial{B}}=\frac{\partial{Loss}}{\partial{A}} * \frac{\partial{A}}{\partial{Z}} * \frac{\partial{Z}}{\partial{B}} \\
=\frac{A-Y}{A(1-A)} * A(1-A) \\
=A-Y \tag{B的梯度}
$$

# 多分类（大于两类）
在输出层使用多个神经元输出，并对多个神经元使用Softmax分类函数和下面这种交叉熵损失函数：

$$
Z = W \times X+B \tag{矩阵运算}
$$
$$
A=Softmax(Z) \tag{多分类函数}
$$
$$
Loss = -Y * lnA \tag{多分类交叉熵函数}
$$

<img src=".\Images\6\NN.jpg">

## Softmax 函数
### 形式与特点

softmax函数，是大名鼎鼎的在计算多分类问题时常使用的一个函数，他长成这个样子:

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}} \tag{对第j项的分类值计算结果}
$$

上式中:
- $z_j$是对第 j 项的分类原始值（即矩阵运算的结果，假设j=1，上图中$z_1=w_{11}x_1+w_{12}x_2+b1$）
- $z_i$是参与分类计算的每个类别的原始值（即矩阵运算的结果，上图中$z_i=w_{i1}x_1+w_{i2}x_2+bi$）
- m 是总的分类数（上图中m=3）
- $a_j$是对第 j 项的计算结果（假设j=1，上图中$a_1=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_1)}{exp(z_1)+exp(z_2)+exp(z_3)}$）

也就是说把接收到的输入归一化成一个每个分量都在$(0,1)$之间并且总和为一的一个概率函数。在上图中，$A1+A2+A3=1$。

分类的方式是：看A1/A2/A3三者谁的比重最大，就算是那一类。比如A1=0.6, A2=0.1, A3=0.3，则这个样本就算是第一类了。

用一张图来形象说明这个过程：

<img src=".\Images\7\softmax.png">

当输入的数据是3，1，-3时（$z_1,z_2,z_3$），按照图示过程进行计算，可以得出输出的概率分布是0.88，0.12，0。

试想如果我们并没有这样一个softmax的过程而是直接根据[3，1，-3]这样的输出，而我们期望得结果是[1，0，0]这样的概率分布结果，那传递给网络的信息是什么呢？

其实[3,1,-3]这一组数字和[1,0,0]是不可比的，就好像用1公里的“1”和1毫米的“1”去比。如果非要比的话，第一类3-1=2，要抑制，但抑制多少呢？第三类-3-0=-3，要鼓励？真的要鼓励吗？想不清楚。所以Softmax函数就有用了，它先归一化所有输入数据到[0,1]之间，然后再和标签值[1,0,0]去比，一是可比了，二是知道要抑制或鼓励的幅度。

从继承关系的角度来说，softmax函数可以视作sigmoid的一个扩展，比如我们来看一个二分类问题，

$$
A_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}} = \frac{1}{1 + e^{z_2} e^{- z_1}}
$$

是不是和sigmoid的函数形式非常像？比起原始的sigmoid函数，softmax的一个优势是可以用在多分类的问题中。另一个好处是在计算概率的时候更符合一般意义上我们认知的概率分布，体现出物体属于各个类别相对的概率大小。

### Softmax求导

由于Softmax涉及到求和，所以有两种情况：
- 当预测的分类项 j 和标签项 i 一致时，$i=j$。比如分类结果是[0.8, 0.1, 0.1]，第一项最大。标签项是[1, 0, 0]，表示当前样本确实是第一类。
- 当预测的分类项 j 和标签项 i 不一致时，$i \neq j$。比如分类结果是[0.1, 0.8, 0.1]，第二项最大。标签项是[1, 0, 0]，表示当前样本应该是第一类。

Softmax函数的分子：因为是计算$a_j$，所以分子是$e^{z_j}$
Softmax函数的分母：
$$
\sum\limits_{i=1}^m e^{z_i} = e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m} = \sum \tag{为了方便简写成$\sum$}
$$

- $i=j$时（比如输出分类值a1对z1的求导），求$a_j$对$z_i$的导数，此时分子上的$e^{z_j}$要参与求导。参考基本数学导数公式33：

$$
\frac{\partial{a_j}}{\partial{z_i}} = \frac{\partial{(e^{z_j}/(e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m}))}}{\partial{z_i}}
$$
$$
= \frac{\partial{}}{\partial{z_i}}(e^{z_j}/\sum)  = \frac{\partial{}}{\partial{z_j}}(e^{z_j}/\sum) \tag{因为$z_i=z_j$}
$$
$$
=\frac{e^{z_j}\sum-e^{z_j}e^{z_j}}{(\sum)^2} =\frac{e^{z_j}}{\sum} - \frac{(e^{z_j})^2}{(\sum)^2} \\
$$
$$
= a_j-a^2_j=a_j(1-a_j)
$$

也就是说，此时softmax的梯度就是$a_j(1-a_j)$。

- $i \neq j$时（比如输出分类值a1对z2的求导，j=1, i=2），$a_j$对$z_i$的导数，分子上的$z_j与i$没有关系，求导为0，分母的求和项中$e^{z_i}$要参与求导。同样是公式33，因为分子$e^{z_j}$对$e^{z_i}$求导的结果是0：

$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}
$$
求和公式对$e^{z_i}$的导数$(\sum)'$，除了$e^{z_i}$项外，其它都是0：
$$
(\sum)' = (e^{z_1} + \dots + e^{z_i} + \dots +e^{z_m})'=e^{z_i}
$$
所以：
$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}=-\frac{e^{z_j}e^{z_i}}{{(\sum)^2}}=-\frac{e^{z_j}}{{\sum}}\frac{e^{z_j}}{{\sum}}=-a_{i}a_{j}
$$

### 结合损失函数的整体反向传播公式

<img src=".\Images\7\Loss-A-Z.jpg">

看上图，假设我们要求Loss值对Z1的偏导数，应该怎么做呢？

先从Loss的公式看，$Loss=-(y_1lna_1+y_2lna_2+y_3lna_3)$，a1肯定与z1有关，那么a2,a3是否与z1有关呢？

再从Softmax函数的形式来看：
$$
a_1=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_1)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
$$
a_2=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_2)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
$$
a_3=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_3)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
无论是a1，a2，a3，都是与z1相关的，而不是一对一的关系，所以，想求Loss对Z1的偏导，必须把Loss->A1->Z1， Loss->A2->Z1，Loss->A3->Z1，这三条路的结果加起来。于是有了如下公式：

$$
\frac{\partial{Loss}}{\partial{z_i}}= \frac{\partial{Loss}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_i}} + \frac{\partial{Loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_i}} + \frac{\partial{Loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_i}}\\
=\sum_j \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}
$$

你可以假设上式中$i=1，j=3$，就完全符合我们的假设了，而且不失普遍性。

前面说过了，因为Softmax涉及到各项求和，A的分类结果和Y的标签值分类是否一致，所以需要分情况讨论：

$$
\frac{\partial{a_j}}{\partial{z_i}} = \begin{cases} a_j(1-a_j), & i = j \\ -a_ia_j, & i \neq j \end{cases}\\
$$

因此，$\frac{\partial{Loss}}{\partial{z_i}}$应该是$i=j和i \neq j$两种情况的和：
- $i = j时，Loss通过A1对Z1求导（或者是通过A2对Z2求导）$：

$$
\frac{\partial{Loss}}{\partial{z_i}} = \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=-\frac{y_j}{a_j}*a_j(1-a_j)=y_j(a_j-1)=y_i(a_i-1) \tag{公式一}
$$

- $i \neq j，Loss通过A2+A3对Z1求导$：

$$
\frac{\partial{Loss}}{\partial{z_i}} = \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=\sum_j^m(-\frac{y_j}{a_j})(-a_ja_i)=\sum_j^m(y_ja_i)=a_i\sum{y_j} \tag{公式二}
$$

如果不好理解的话，就拆开上面的公式，假设求Loss通过a2/a3对z1的导数：

$$
\frac{\partial{Loss}}{\partial{z_1}}=\frac{\partial{Loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_1}}+\frac{\partial{Loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_1}} \\
$$
因为：
$$
\frac{\partial{Loss}}{\partial{a_2}}=-\frac{y_2}{a_2} \\
\frac{\partial{a_2}}{\partial{z_1}}=-a_2a_1 \\
\frac{\partial{Loss}}{\partial{a_3}}=-\frac{y_3}{a_3} \\
\frac{\partial{a_3}}{\partial{z_1}}=-a_3a_1
$$
所以：
$$
\frac{\partial{Loss}}{\partial{z_1}} = (-\frac{y_2}{a_2})(-a_2a_1)+(-\frac{y_3}{a_3})(-a_3a_1)=y_2a_1+y_3a_1=a1(y_2+y_3)=a_i\sum_{j\neq i}y_j
$$

把两种情况加起来：
$$
\frac{\partial{Loss}}{\partial{z_i}} = y_i(a_i-1)+a_i\sum_{j \neq i}y_j
$$
$$
=-y_i+a_iy_i+a_i\sum_{j \neq i}y_j =-y_i+a_i(y_i+\sum_{j \neq i}y_j)
$$
$$
=-y_j + a_i\sum_jy_j =-y_j + a_i*1=a_i-y_j
$$
因为$y_j$是取值[1,0,0]或者[0,1,0]或者[0,0,1]的，这三者用$\sum$加起来，就是[1,1,1]，在矩阵乘法运算里乘以[1,1,1]相当于什么都不做，就等于原值。

# Softmax函数的Python实现

第一种，直截了当按照公式写：
```Python
def Softmax1(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```
这个可能会发生的问题是，当x很大时，np.exp(x)很容易溢出，因为是指数运算。所以，有了下面这种改进的代码：

```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
测试一下：
```Python
Z = np.array([3,0,-3])
print(Softmax1(Z))
print(Softmax2(Z))
```
两个实现方式的结果一致：
```
[0.95033021 0.04731416 0.00235563]
[0.95033021 0.04731416 0.00235563]
```

为什么一样呢？从表面上看差好多啊！我们来证明一下：

假设有3个值a，b，c，并且a在三个数中最大，则b所占的Softmax比重应该这样写：

$$P(b)=\frac{e^b}{e^a+e^b+e^c}$$

如果减去最大值变成了a-a，b-a，c-a，则b'所占的Softmax比重应该这样写：

$$P(b') = \frac{e^{b-a}}{e^{a-a}+e^{b-a}+e^{c-a}}\\ 
=\frac{e^b/e^a}{e^a/e^a+e^b/e^a+e^c/e^a} \\
= \frac{e^b}{e^a+e^b+e^c}
$$
$$
所以：P(b) == P(b')
$$

Softmax2的写法对一个一维的向量或者数组是没问题的，如果遇到Z是个MxN维(M,N>1)的矩阵的话，就有问题了，因为np.sum(exp_Z)这个函数，会把MxN矩阵里的所有元素加在一起，得到一个标量值，而不是相关列元素加在一起。

所以应该这么写：

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    #s = np.sum(exp_z)
    s = np.sum(exp_z, axis=0)
    A = exp_z / s
    return A
```

axis=0这个参数非常非常重要，具体原因在“6-扩展阅读”这篇文章里，这里假设我们的数据矩阵中，每个列是一个样本的N个特征值。如果是每个行是特征值组合的话，则axis=1。


